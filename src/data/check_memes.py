import json
import re

def check_and_clean_duplicates(input_file="memes.json", output_file="memes_cleaned.json"):
    # 1. è¯»å– JSON æ•°æ®
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"ğŸ“¥ æˆåŠŸåŠ è½½æ•°æ®ï¼Œå…± {len(data)} æ¡ã€‚å¼€å§‹æ™ºèƒ½æŸ¥é‡...\n")
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨: {e}")
        return

    # 2. åœç”¨è¯åº“ï¼šé˜²æ­¢å¸¸è§çš„åŠ©è¯æˆ–æ— æ„ä¹‰åç¼€å¼•å‘è¯¯åˆ¤
    stopwords = {'çš„', 'äº†', 'æ˜¯', 'æˆ‘', 'ä½ ', 'ä»–', 'å•Š', 'å§', 'å‘€', 'ä¸', 'ç‰ˆ', 'å¼', 'ç¬¬', 'å¤§', 'å°'}
    
    duplicate_pairs = []
    to_remove_ids = set() # è®°å½•éœ€è¦å‰”é™¤çš„é‡å¤é¡¹ ID

    # 3. æ ¸å¿ƒæŸ¥é‡é€»è¾‘ï¼šåŒé‡å¾ªç¯æ¯”å¯¹
    for i in range(len(data)):
        item1 = data[i]
        for j in range(i + 1, len(data)):
            item2 = data[j]
            
            term1 = item1.get('term', '')
            term2 = item2.get('term', '')

            # åˆ©ç”¨æ­£åˆ™ä»…æå–ä¸­æ–‡å­—ç¬¦ï¼Œå¹¶è¿‡æ»¤æ‰åœç”¨è¯
            chars1 = set(re.findall(r'[\u4e00-\u9fa5]', term1)) - stopwords
            chars2 = set(re.findall(r'[\u4e00-\u9fa5]', term2)) - stopwords
            
            # æ±‚ä¸¤ä¸ªè¯æ¡åŒ…å«çš„å…±åŒå­—ç¬¦äº¤é›†
            common_chars = chars1 & chars2
            
            # ğŸ¯ åˆ¤å®šæ¡ä»¶ï¼šæœ‰ä¸¤ä¸ªå­—ç¬¦ï¼ˆæˆ–ä»¥ä¸Šï¼‰ç›¸åŒ
            if len(common_chars) >= 2:
                duplicate_pairs.append({
                    't1': f"{term1} (ID:{item1['id']})",
                    't2': f"{term2} (ID:{item2['id']})",
                    'common': "".join(common_chars)
                })
                # å°†åå‡ºç°çš„è¡ç”Ÿè¯æ¡ï¼ˆIDé€šå¸¸è¾ƒå¤§ï¼‰æ ‡è®°ä¸ºå¾…åˆ é™¤
                to_remove_ids.add(item2['id'])

    # 4. è¾“å‡ºæŸ¥é‡æŠ¥å‘Š
    if not duplicate_pairs:
        print("âœ… å®Œç¾ï¼æ²¡æœ‰å‘ç°åŒ…å«ä¸¤ä¸ªç›¸åŒå­—ç¬¦çš„é‡å¤é¡¹ã€‚")
    else:
        print(f"âš ï¸ æ‰«æå‘ç° {len(duplicate_pairs)} å¯¹ç–‘ä¼¼é‡å¤é¡¹ï¼ˆå·²åŒ¹é…â€œè‡³å°‘ä¸¤ä¸ªå­—ç¬¦ç›¸åŒâ€ï¼‰ï¼š")
        for idx, pair in enumerate(duplicate_pairs, 1):
            print(f"  [{idx}] ã€{pair['t1']}ã€‘ <==> ã€{pair['t2']}ã€‘ (é‡å¤å­—ç¬¦: '{pair['common']}')")
        
        # 5. è‡ªåŠ¨å‰”é™¤é‡å¤é¡¹å¹¶ç”Ÿæˆæ–°æ–‡ä»¶
        cleaned_data = [item for item in data if item['id'] not in to_remove_ids]
        
        print("\n" + "="*40)
        print(f"ğŸ—‘ï¸ è‡ªåŠ¨å»é‡å¤„ç†å®Œæ¯•ï¼š")
        print(f"  - åŸæœ‰è¯æ¡æ€»æ•°: {len(data)}")
        print(f"  - è‡ªåŠ¨å‰”é™¤æ•°é‡: {len(to_remove_ids)}")
        print(f"  - å‰©ä½™ç‹¬ç«‹è¯æ¡: {len(cleaned_data)}")
        print("="*40)
        
        # å°†æ¸…ç†åçš„å¹²å‡€æ•°æ®ä¿å­˜ä¸‹æ¥
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(cleaned_data, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ å¹²å‡€æ— é‡å¤çš„æ•°æ®å·²è‡ªåŠ¨ä¿å­˜è‡³: {output_file}")
        print("ğŸ’¡ æç¤ºï¼šä½ å¯ä»¥ç›´æ¥ä½¿ç”¨ä¸Šæ–‡æˆ‘å‘ç»™ä½ çš„ã€30ä¸ªå…¨æ–°æ›¿æ¢åŒ…ã€‘è¡¥å……ä¸è¶³çš„æ¡ç›®ï¼")

if __name__ == "__main__":
    check_and_clean_duplicates()